{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrXQG3S5O1i3",
        "colab_type": "text"
      },
      "source": [
        "# Training GPT-2 from scratch\n",
        "This notebook trains GPT-2 from scratch in COLAB.\n",
        "\n",
        "Also, as an experiment, GPT-2 model with tied weights in across transformed blocks is trained.\n",
        "The intuition for that was, that weight tying might be benefitial, so that rules learned in one transformer block can be re-used in other blocks.\n",
        "\n",
        "Here some results obtained:\n",
        "*   Validation loss for vanilla GPT-2: 3.2658\n",
        "*   Validation loss for GPT-2 with tied parameters in attention blocks: 3.5591\n",
        "\n",
        "Experimental model has higher loss, but 6x less parameters in transformer block layers.\n",
        "\n",
        "## Dataset and implementation details\n",
        "* Dataset is wikitext-2\n",
        "* GPT-2 model implementaion is taken from https://github.com/lopuhin/transformer-lm and adapted to Colab environment. Added code for weight tying experiment.\n",
        "\n",
        "# References\n",
        "* \"Language Models are Unsupervised Multitask Learners\", https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "* \"Attention is all you need\", https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "## Steps to reproduce\n",
        "Colab sessions are time-limited and may termitate due to network issues. Because of this, training data and model are kept on the mounted Google Drive.\n",
        "Choose the base path on the Google Drive and modify base_path below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ygFL4Zr5W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_path='drive/My Drive/Colab Notebooks/lopuhin_transformer_lm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTt65tlRUT1m",
        "colab_type": "text"
      },
      "source": [
        "# Upload dataset\n",
        "* Create sub-folder 'data' in the base folder, and upoad train.txt, test.txt and valid.txt from wiki-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TKiMXwnsD6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/semicontinuity/nlp.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCj6xJ9uIF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!(cd nlp; git pull --update; git reset --hard)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faM6B7UmuPGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install attr json-log-plots fire matplotlib numpy sentencepiece torch tqdm\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8mXUUlmU2cW",
        "colab_type": "text"
      },
      "source": [
        "# Create input dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6NVsCz7VI3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = base_path + '/data-'\n",
        "\n",
        "from nlp.lopuhin_transformer_lm.lm import data\n",
        "data.sp_train(\n",
        "    data_path,\n",
        "    data_path + '/sp-text.txt',\n",
        "    data_path + '/sp-model',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIYXGen6xbxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1py7azIbW_wJ",
        "colab_type": "text"
      },
      "source": [
        "# Train/Evaluale model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gokPpzdkoP2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "work_path=data_path + '/work'\n",
        "work_path=data_path + '/work-tied-blocks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcAQaEGXEaJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nlp.lopuhin_transformer_lm.lm import main\n",
        "\n",
        "only_validate = True\n",
        "tied_blocks = True\n",
        "\n",
        "main.main(\n",
        "        run_path      = work_path,\n",
        "        dataset_path  = 'drive/My Drive/Colab Notebooks/lopuhin_transformer_lm/data/encoded',\n",
        "        sp_model_path = 'drive/My Drive/Colab Notebooks/lopuhin_transformer_lm/data/sp-model.model',\n",
        "        batch_size    = 26,  # per GPU: 36 if 16GB GPU RAM available, 26 if 11.4\n",
        "        epochs        = 10,\n",
        "        g_accum_gradients        = 2,  # accumulate gradients N times (globally)\n",
        "        gradient_checkpointing   = False, # saves GPU memory\n",
        "        n_ctx         = 256,\n",
        "        n_embed       = 512,\n",
        "        n_head        = 4,\n",
        "        n_layer       = 6,\n",
        "        epoch_pbar_refresh_every = 50,\n",
        "        log_every     = 1000,\n",
        "        save_every    = 1000,\n",
        "        validate_every= 1000,\n",
        "        only_validate = only_validate,\n",
        "        tied_blocks   = tied_blocks,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itaxS57HbyRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsDdVJ30nHMZ",
        "colab_type": "text"
      },
      "source": [
        "*   Validation loss for vanilla GPT-2: 3.2658\n",
        "*   Validation loss for GPT-2 with tied parameters in attention blocks: 3.5591"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYtcQI9xnLYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}